{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compas Data Imputation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import create_compas_dataset, Dataset\n",
    "from utils.generator import gen_complete_random\n",
    "from utils.completer import complete_by_mean_col, complete_by_multi, complete_by_similar_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_compas_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_compas_complete = data.copy()\n",
    "tmp_concat = pd.concat([data_compas_complete.X, pd.DataFrame(data_compas_complete.y, columns=[\"_TARGET_\"])], axis=1)\n",
    "tmp_concat.dropna(inplace=True)\n",
    "tmp_concat.reset_index(drop=True, inplace=True)\n",
    "data_compas_complete.X = tmp_concat.drop(columns=[\"_TARGET_\"]).copy()\n",
    "data_compas_complete.y = tmp_concat[\"_TARGET_\"].copy().to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_compas_complete.X.drop(columns=data_compas_complete.protected).copy().to_numpy()\n",
    "y = data_compas_complete.y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(X, y, model, params, smote):\n",
    "    print(\"Model: {}\".format(model.__class__.__name__))\n",
    "    X_res, y_res = smote.fit_resample(X, y) # enlarge dataset\n",
    "    search = GridSearchCV(model, param_grid=params, cv=10)\n",
    "    search.fit(X_res, y_res)\n",
    "    print(\"Best parameter: {}\".format(search.best_params_))\n",
    "    print(\"Acc best: {:.4f}\".format(search.best_score_))\n",
    "    print(\"Acc on input data: {:.4f}\".format(search.best_estimator_.score(X, y)))\n",
    "    print(\"Acc on enlarged data: {:.4f}\".format(search.best_estimator_.score(X_res, y_res)))\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "    \"KNN\": None,\n",
    "    \"LinearSVC\": None,\n",
    "    \"SVC\": None,\n",
    "    \"Forest\": None,\n",
    "    \"LogReg\": None,\n",
    "    \"Tree\": None,\n",
    "    \"MLP\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: KNeighborsClassifier\n",
      "Best parameter: {'leaf_size': 10, 'n_neighbors': 2, 'weights': 'uniform'}\n",
      "Acc on input data: 0.8205\n",
      "Acc on enlarged data: 0.8629\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"n_neighbors\": [2, 5, 10, 50, 100, 200, 500],\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"leaf_size\": [10, 30, 100],\n",
    "}\n",
    "all_params[\"KNN\"] = grid_search(X, y, KNeighborsClassifier(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC\n",
      "Best parameter: {'C': 0.01, 'max_iter': 1000, 'tol': 0.001}\n",
      "Acc on input data: 0.6642\n",
      "Acc on enlarged data: 0.6649\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "}\n",
    "all_params[\"LinearSVC\"] = grid_search(X, y, LinearSVC(dual=False), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVC\n",
      "Best parameter: {'C': 0.01, 'max_iter': 10000, 'tol': 1e-05}\n",
      "Acc on input data: 0.6474\n",
      "Acc on enlarged data: 0.7330\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"max_iter\": [1000, 5000, 10000, -1],\n",
    "}\n",
    "all_params[\"SVC\"] = grid_search(X, y, SVC(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "Best parameter: {'max_depth': 50, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "Acc on input data: 0.8158\n",
      "Acc on enlarged data: 0.8605\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"n_estimators\": [50, 100, 200, 500],\n",
    "    \"max_depth\": [None, 10, 50, 100],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "}\n",
    "all_params[\"Forest\"] = grid_search(X, y, RandomForestClassifier(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Best parameter: {'C': 1, 'max_iter': 100, 'tol': 1e-05}\n",
      "Acc on input data: 0.6707\n",
      "Acc on enlarged data: 0.6703\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3],\n",
    "    \"C\": [1e-2, 1e-1, 1, 1e1, 1e2],\n",
    "    \"max_iter\": [100, 500, 1000, 2000],\n",
    "}\n",
    "all_params[\"LogReg\"] = grid_search(X, y, LogisticRegression(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      "Best parameter: {'max_depth': 10, 'max_leaf_nodes': 100, 'min_samples_leaf': 10}\n",
      "Acc on input data: 0.7263\n",
      "Acc on enlarged data: 0.7909\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"max_depth\": [None, 10, 50, 100, 200],\n",
    "    \"max_leaf_nodes\": [None, 10, 100, 1000],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "}\n",
    "all_params[\"Tree\"] = grid_search(X, y, DecisionTreeClassifier(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLPClassifier\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    \"hidden_layer_sizes\": [(10,), (50,), (100,), (200,), (500,)],\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3],\n",
    "    \"learning_rate_init\": [1e-4, 1e-3, 1e-2],\n",
    "    \"max_iter\": [100, 200, 500, 1000],\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3],\n",
    "}\n",
    "all_params[\"MLP\"] = grid_search(X, y, MLPClassifier(), tmp_params, KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def helper_freq(array):\n",
    "    \"\"\"simple helper function to return the most frequent number in an array\"\"\"\n",
    "    count = np.bincount(array)\n",
    "    return array[np.argmax(count)]\n",
    "\n",
    "def average_cv(cv_data):\n",
    "    # compute average for the confusion matrix data for each fold\n",
    "    result = {}\n",
    "    for name, data in cv_data.items():\n",
    "        new_data = {\n",
    "            \"African-American\": np.array([m[\"African-American\"] for m in data]).mean(axis=0).tolist(),\n",
    "            \"Caucasian\": np.array([m[\"Caucasian\"] for m in data]).mean(axis=0).tolist()\n",
    "        }\n",
    "        result[name] = new_data\n",
    "    return result\n",
    "\n",
    "def compute_confusion_matrix(X_train, y_train, X_test, y_test, clf, protected_features, multi=False):\n",
    "    # X are pandas dataframe\n",
    "    # y are numpy array\n",
    "    # clf is a sklearn classifier\n",
    "    # protected_features is list\n",
    "    smote = KMeansSMOTE(cluster_balance_threshold=0.4, random_state=22)\n",
    "    if not multi:\n",
    "        X_train = X_train.drop(columns=protected_features).copy().to_numpy()\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        X_test_AA = X_test[X_test[\"race\"] == \"African-American\"].drop(columns=protected_features).to_numpy()\n",
    "        X_test_C = X_test[X_test[\"race\"] == \"Caucasian\"].drop(columns=protected_features).to_numpy()\n",
    "        y_test_AA = y_test[X_test[X_test[\"race\"] == \"African-American\"].index.tolist()]\n",
    "        y_test_C = y_test[X_test[X_test[\"race\"] == \"Caucasian\"].index.tolist()]\n",
    "        matrix_AA = confusion_matrix(y_test_AA, clf.predict(X_test_AA))\n",
    "        matrix_C = confusion_matrix(y_test_C, clf.predict(X_test_C))\n",
    "    else:\n",
    "        prediction_AA = []\n",
    "        prediction_C = []\n",
    "        X_test_first = X_test[0]\n",
    "        y_test_AA = y_test[X_test_first[X_test_first[\"race\"] == \"African-American\"].index.tolist()]\n",
    "        y_test_C = y_test[X_test_first[X_test_first[\"race\"] == \"Caucasian\"].index.tolist()]\n",
    "        for X_train_m in X_train:\n",
    "            X_train_m = X_train_m.drop(columns=protected_features).copy().to_numpy()\n",
    "            X_train_res, y_train_res = smote.fit_resample(X_train_m, y_train)\n",
    "            clf.fit(X_train_res, y_train_res)\n",
    "            for X_test_m in X_test:\n",
    "                X_test_AA = X_test_m[X_test_m[\"race\"] == \"African-American\"].drop(columns=protected_features).to_numpy()\n",
    "                X_test_C = X_test_m[X_test_m[\"race\"] == \"Caucasian\"].drop(columns=protected_features).to_numpy()\n",
    "                prediction_AA.append(clf.predict(X_test_AA))\n",
    "                prediction_C.append(clf.predict(X_test_C))\n",
    "        # compute final predictions by voting\n",
    "        prediction_AA = np.apply_along_axis(helper_freq, 0, np.array(prediction_AA))\n",
    "        prediction_C = np.apply_along_axis(helper_freq, 0, np.array(prediction_C))\n",
    "    result = {\n",
    "        \"African-American\": matrix_AA.ravel().tolist(), # [tn, fp, fn, tp]\n",
    "        \"Caucasian\": matrix_C.ravel().tolist()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def test_imputation(X, y, protected_features, completer_func, multi=False):\n",
    "    # X is pandas dataframe\n",
    "    # y is numpy array,\n",
    "    # protected_features is list\n",
    "    # completer func is the imputation function\n",
    "    global all_params\n",
    "    clfs = { # define all the classifiers with best parameters\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=all_params[\"KNN\"][\"n_neighbors\"], weights=all_params[\"KNN\"][\"weights\"], leaf_size=all_params[\"KNN\"][\"leaf_size\"]),\n",
    "        \"LinearSVC\": LinearSVC(dual=False, tol=all_params[\"LinearSVC\"][\"tol\"], C=all_params[\"LinearSVC\"][\"C\"], max_iter=all_params[\"LinearSVC\"][\"max_iter\"]),\n",
    "        \"SVC\": SVC(tol=all_params[\"SVC\"][\"tol\"], C=all_params[\"SVC\"][\"C\"], max_iter=all_params[\"SVC\"][\"max_iter\"]),\n",
    "        \"Forest\": RandomForestClassifier(n_estimators=all_params[\"Forest\"][\"n_estimators\"], max_depth=all_params[\"Forest\"][\"max_depth\"], min_samples_leaf=all_params[\"Forest\"][\"min_samples_leaf\"]),\n",
    "        \"LogReg\": LogisticRegression(tol=all_params[\"LogReg\"][\"tol\"], C=all_params[\"LogReg\"][\"C\"], max_iter=all_params[\"LogReg\"][\"max_iter\"]),\n",
    "        \"Tree\": DecisionTreeClassifier(max_depth=all_params[\"Tree\"][\"max_depth\"], max_leaf_nodes=all_params[\"Tree\"][\"max_leaf_nodes\"], min_samples_leaf=all_params[\"Tree\"][\"min_samples_leaf\"]),\n",
    "        \"MLP\": MLPClassifier(hidden_layer_sizes=all_params[\"MLP\"][\"hidden_layer_sizes\"], alpha=all_params[\"MLP\"][\"alpha\"], learning_rate_init=all_params[\"MLP\"][\"learning_rate_init\"], max_iter=all_params[\"MLP\"][\"max_iter\"], tol=all_params[\"MLP\"][\"tol\"]),\n",
    "    }\n",
    "    data_cv = { # save each cv output\n",
    "        \"KNN\": [],\n",
    "        \"LinearSVC\": [],\n",
    "        \"SVC\": [],\n",
    "        \"Forest\": [],\n",
    "        \"LogReg\": [],\n",
    "        \"Tree\": [],\n",
    "        \"MLP\": [],\n",
    "    }\n",
    "    kf = KFold(n_splits=10)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        # do imputations on training set and test set individually\n",
    "        data_incomplete = Dataset(X_train, y_train, auto_convert=False, protected_features=protected_features)\n",
    "        data_complete = completer_func(data_incomplete)\n",
    "        X_train = [m.X.copy() for m in data_complete] if multi else data_complete.X.copy() \n",
    "        y_train = data_complete[0].y.copy() if multi else data_complete.y.copy()\n",
    "        data_incomplete = Dataset(X_test, y_test, auto_convert=False, protected_features=protected_features)\n",
    "        data_complete = completer_func(data_incomplete)\n",
    "        X_test = [m.X.copy() for m in data_complete] if multi else data_complete.X.copy()\n",
    "        y_test = data_complete[0].y.copy() if multi else data_complete.y.copy()\n",
    "        # get result for each classifier\n",
    "        for clf_name, clf in clfs.items():\n",
    "            result = compute_confusion_matrix(X_train, X_test, y_train, y_test, clf, protected_features, multi=multi)\n",
    "            data_cv[clf_name].append(result)\n",
    "    return data_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
